import json
import base64
from pathlib import Path
from io import BytesIO
from typing import Dict, List, Union
from PIL import Image
import openai
import argparse

def load_prompts(prompt_dir: Path) -> Dict[str, str]:
    """í”„ë¡¬í”„íŠ¸ í…ìŠ¤íŠ¸ íŒŒì¼(.txt)ì„ ëª¨ë‘ ì½ì–´ ë”•ì…”ë„ˆë¦¬ë¡œ ë°˜í™˜"""
    prompts = {}
    for file in prompt_dir.glob("*.txt"):
        prompts[file.stem] = file.read_text(encoding="utf-8").strip()
    return prompts

def crop_and_encode(
    image_path: Path,
    bbox: List[int]
) -> str:
    """ì´ë¯¸ì§€ ê²½ë¡œì™€ bbox ë¦¬ìŠ¤íŠ¸(x1,y1,x2,y2)ë¥¼ ë°›ì•„ í¬ë¡­ í›„ Base64ë¡œ ì¸ì½”ë”©í•˜ì—¬ ë°˜í™˜"""
    x1, y1, x2, y2 = bbox
    with Image.open(image_path) as img:
        cropped = img.crop((x1, y1, x2, y2))
        buffer = BytesIO()
        cropped.save(buffer, format="PNG")
        return base64.b64encode(buffer.getvalue()).decode("utf-8")

def evaluate(
    image_b64: str,
    prompt: str,
    api_key: str
) -> str:
    """GPTì— ì´ë¯¸ì§€(Base64)ì™€ í”„ë¡¬í”„íŠ¸ë¥¼ í•¨ê»˜ ì „ì†¡"""
    client = openai.OpenAI(api_key=api_key)
    messages = [
        {
            "role": "user",
            "content": [
                {"type": "text", "text": prompt},
                {
                    "type": "image_url",
                    "image_url": {
                        "url": f"data:image/png;base64,{image_b64}"
                    }
                }
            ]
        }
    ]
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=messages
    )
    return response.choices[0].message.content


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="ChartParser: layoutparser ê²°ê³¼ JSONì—ì„œ bboxë¥¼ ë°›ì•„ ì´ë¯¸ì§€ í¬ë¡­ í›„ GPT ë¶„ì„ ìš”ì²­"
    )
    parser.add_argument("--layout_json", type=Path, required=True,
                        help="layoutparserê°€ ìƒì„±í•œ JSON íŒŒì¼ ê²½ë¡œ (bbox ë¦¬ìŠ¤íŠ¸ í¬í•¨)")
    parser.add_argument("--images_dir", type=Path, required=True,
                        help="layoutparserê°€ ì €ì¥í•œ í˜ì´ì§€ ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬")
    parser.add_argument("--prompts_dir", type=Path, required=True,
                        help="í”„ë¡¬í”„íŠ¸ í…ìŠ¤íŠ¸ íŒŒì¼(.txt) ë””ë ‰í† ë¦¬")
    parser.add_argument("--api_key", type=str, required=True,
                        help="OpenAI API í‚¤")
    args = parser.parse_args()

    # 1) layoutparser ê²°ê³¼ JSON ë¡œë“œ
    with open(args.layout_json, "r", encoding="utf-8") as f:
        detections: List[Dict] = json.load(f)
    print(f"ğŸ” Loaded {len(detections)} detections from {args.layout_json}")
    if not detections:
        print("âš ï¸ No detections found. layout_json ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        exit(1)

    # 2) í”„ë¡¬í”„íŠ¸ ë¡œë“œ
    prompts = load_prompts(args.prompts_dir)
    base_prompt = prompts.get("chart_analysis", "ì°¨íŠ¸ ë‚´ìš©ì„ ë¶„ì„í•´ì¤˜.")

    # 3) ë””ë²„ê·¸: ì²˜ìŒ 5ê°œ ë””í…ì…˜ í™•ì¸
    for idx, det in enumerate(detections[:5]):
        print(f"â¡ï¸ det[{idx}]: page={det.get('page_num')}, label={det.get('label')}, bbox={det.get('bbox')}")

    # 4) ê° detectionì— ëŒ€í•´ ì´ë¯¸ì§€ í¬ë¡­ ë° GPT í˜¸ì¶œ
    results = []
    for det in detections:
        page = det.get("page_num")
        bbox = det.get("bbox", [])
        doc_name = det.get("document_name", f"page_{page}")

        img_file = args.images_dir / f"page_{page}.jpg"
        if not img_file.exists():
            print(f"âŒ Missing image file: {img_file}")
            continue
        if not bbox or len(bbox) != 4:
            print(f"âŒ Invalid bbox for det: {det}")
            continue

        # ì´ë¯¸ì§€ í¬ë¡­ ë° GPT í˜¸ì¶œ
        image_b64 = crop_and_encode(img_file, bbox)
        prompt = base_prompt
        result = evaluate(image_b64, prompt, args.api_key)

        print(f"ğŸ¯ Page {page} {doc_name} ì°¨íŠ¸ ë¶„ì„ ê²°ê³¼: {result}")

        results.append({
            "page": page,
            "document_name": doc_name,
            "bbox": bbox,
            "prompt_type": "chart_analysis",
            "result": result
        })

    # 5) ê²°ê³¼ ì €ì¥
    output_path = Path("llm_a_outputs") / f"{doc_name}.json"
    output_path.parent.mkdir(exist_ok=True, parents=True)

    with open(output_path, "w", encoding="utf-8") as f:
        json.dump({"results": results}, f, indent=2, ensure_ascii=False)
        print(f"âœ… ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_path}")
