import json
import argparse
from pathlib import Path
from typing import Dict
import openai

def load_chart_data(chart_id: str, charts_dir: Path):
    with open(charts_dir / f"{chart_id}.data.json", "r", encoding="utf-8") as f:
        chart_data = json.load(f)
    with open(charts_dir / f"{chart_id}.info.json", "r", encoding="utf-8") as f:
        chart_info = json.load(f)
    return chart_data, chart_info

def load_llm_a_outputs(chart_id: str, outputs_dir: Path) -> Dict[str, str]:
    path = outputs_dir / f"{chart_id}.json"
    if not path.exists():
        raise FileNotFoundError(f"âŒ LLM A ê²°ê³¼ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {path}")
    
    with open(path, "r", encoding="utf-8") as f:
        data = json.load(f)

    results = data.get("results", {})

    # ë¦¬ìŠ¤íŠ¸ë¡œ ë˜ì–´ ìˆìœ¼ë©´ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
    if isinstance(results, list):
        print("ğŸ“Œ 'results'ëŠ” ë¦¬ìŠ¤íŠ¸ í˜•ì‹ì…ë‹ˆë‹¤. ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜ ì¤‘...")
        results = {
            item["prompt_type"]: item["result"]
            for item in results
            if "prompt_type" in item and "result" in item
        }

    if not results:
        print("âš ï¸ 'results'ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. LLM A ê²°ê³¼ê°€ ì—†ê±°ë‚˜ êµ¬ì¡°ê°€ ì˜ëª»ëì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    else:
        print(f"ğŸ“„ LLM A ê²°ê³¼ ë¡œë”© ì™„ë£Œ. í”„ë¡¬í”„íŠ¸ ìœ í˜•: {list(results.keys())}")

    return results

def build_llm_b_prompt(chart_data, chart_info, prompt_type: str, llm_a_output: str) -> str:
    return f"""
ë‹¹ì‹ ì€ ì°¨íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” ë° ì‚¬ìš©ëœ ë°ì´í„°ì™€ ì°¨íŠ¸ ë©”íƒ€ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ,
ë‹¤ìŒ í•´ì„ì´ ì •í™•í•œì§€ ê²€ì¦í•˜ëŠ” ì—­í• ì„ ë§¡ì•˜ìŠµë‹ˆë‹¤.

[ì°¨íŠ¸ ìƒì„± ë°ì´í„°]
{json.dumps(chart_data, ensure_ascii=False, indent=2)}

[ì°¨íŠ¸ ë©”íƒ€ ì •ë³´]
{json.dumps(chart_info, ensure_ascii=False, indent=2)}

[LLM Aì˜ ì°¨íŠ¸ í•´ì„ ê²°ê³¼]
{llm_a_output}

[ê²€ì¦ ê¸°ì¤€: {prompt_type}]
ìœ„ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ í•´ì„ ê²°ê³¼ê°€ ì •í™•í•œì§€ í‰ê°€í•´ì£¼ì„¸ìš”.
ê°€ëŠ¥í•˜ë‹¤ë©´ 'ì •í™•í•¨', 'ë¶€ë¶„ì ìœ¼ë¡œ ì •í™•í•¨', 'ë¶€ì •í™•í•¨' ì¤‘ í•˜ë‚˜ë¡œ íŒë‹¨í•˜ê³  ê°„ë‹¨í•œ ê·¼ê±°ë¥¼ ì œì‹œí•´ì£¼ì„¸ìš”.
"""

def evaluate_with_llm_b(prompt: str, api_key: str) -> str:
    client = openai.OpenAI(api_key=api_key)
    messages = [{"role": "user", "content": prompt}]
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=messages
    )
    return response.choices[0].message.content

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="LLM B í‰ê°€ ì‹œìŠ¤í…œ")
    parser.add_argument("--chart_id", type=str, required=True, help="ì°¨íŠ¸ ID (ì˜ˆ: PMC123456)")
    parser.add_argument("--charts_dir", type=Path, required=True, help=".data.json, .info.jsonì´ ì €ì¥ëœ ë””ë ‰í† ë¦¬")
    parser.add_argument("--outputs_dir", type=Path, required=True, help="LLM A ê²°ê³¼ê°€ ì €ì¥ëœ ë””ë ‰í† ë¦¬")
    parser.add_argument("--prompt_type", type=str, default=None, help="í”„ë¡¬í”„íŠ¸ ìœ í˜• í•„í„° (ì˜ˆ: chart_analysis)")
    parser.add_argument("--api_key", type=str, required=True, help="OpenAI API í‚¤")
    args = parser.parse_args()

    print(f"ğŸ” ì°¨íŠ¸ ID: {args.chart_id}")
    print(f"ğŸ“ charts_dir: {args.charts_dir}")
    print(f"ğŸ“ outputs_dir: {args.outputs_dir}")
    print(f"ğŸ” í‰ê°€ ëŒ€ìƒ prompt_type: {args.prompt_type or 'ëª¨ë‘'}")

    # ë°ì´í„° ë¡œë“œ
    chart_data, chart_info = load_chart_data(args.chart_id, args.charts_dir)
    llm_a_outputs = load_llm_a_outputs(args.chart_id, args.outputs_dir)

    if not llm_a_outputs:
        print("âš ï¸ í‰ê°€í•  LLM A ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        exit(1)

    # í‰ê°€ ì‹œì‘
    for prompt_type, llm_a_output in llm_a_outputs.items():
        if args.prompt_type and prompt_type != args.prompt_type:
            continue
        print(f"\nğŸ§ª [{prompt_type}] í•­ëª© í‰ê°€ ì¤‘...")
        prompt = build_llm_b_prompt(chart_data, chart_info, prompt_type, llm_a_output)
        result = evaluate_with_llm_b(prompt, args.api_key)
        print(f"âœ… í‰ê°€ ê²°ê³¼:\n{result}")
